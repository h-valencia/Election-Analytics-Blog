<!DOCTYPE html>
<html lang="en-us">
    <head>
		
		
		<meta charset="UTF-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>Post-Election Reflection &middot; My New Hugo Site</title>

		
		<link rel="stylesheet" href="/css/style.css">
		<link rel="stylesheet" href="/css/fonts.css">
		
		<link rel="icon" href="/favicon.ico"/>
		<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
		<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

		
		<link href="" rel="alternate" type="application/rss+xml" title="My New Hugo Site" />

		<script src="/js/darkmode.js"></script>
	</head>

    <body>
        		<nav class="nav">
			<div class="nav-container">
				<a href="/">
					
						<h2 class="nav-title">My New Hugo Site</h2>
					
				</a>
				<ul>
    
    
</ul>
			</div>
		</nav>




    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
        <div id="darkModeToggle" onclick="toggleDarkMode()">
  &#9680; 
</div>

        

<main>
	


        <div class="post">
		<div class="post-info">
    <span>Written by</span>
        Hannah Valencia
        <br>
        <span>on&nbsp;</span><time datetime="2022-11-18 00:00:00 &#43;0000 UTC">November 18, 2022</time>
</div>

		<h1 class="post-title">Post-Election Reflection</h1>
<div class="post-line"></div>

		

		


<p><em>This blog is part of a series related to Gov 1347: Election Analytics, a course at <a href="https://www.harvard.edu/">Harvard University</a> taught by Professor <a href="http://ryandenos.com/">Ryan D. Enos</a></em>.</p>
<p><em>On Tuesday, November 8th, eligible Americans voted in the midterm elections for candidates who will make up the 118th Congress. Prior to the election, we had created forecasting models in an attempt to determine who would win the House of Representatives, at the district-level, national-level, or both. In this post I will reflect on the predictions made, my model’s performance, and comment on the election overall.</em></p>
<p>Using the knowledge I gained this semester about election forecasting, I created multiple models to predict the outcome of the 2022 House of Representatives election. I looked at both seat share and vote share, and created two models for each: one using data across all election years, and one using the data for only midterm years. We had the opportunity to work with both national-level and seat-level data, but all of my final models used the data at the national-level for reasons I outlined in my <a href="https://h-valencia.github.io/Election-Analytics-Blog/post/2022-11-04-final-prediction/">Final Prediction Blog</a>.</p>
<p>As a refresher, here are the predictions from my four models and the actual results of the election as of November 21st, 2022:</p>
<p><strong>Model 1:</strong> Democratic Vote Share using all years
yhat = 50.20% , 95% CI (47.86, 52.54)
y = 47.59%</p>
<p><strong>Model 2:</strong> Democratic Seat Share using all years
yhat = 50.51% , 95% CI (45.71, 55.31)
y = 49.30%</p>
<p><strong>Model 3:</strong> Democratic Vote Share using midterm years
yhat = 47.86% , 95% CI (45.81, 49.90)
y = 47.59%</p>
<p><strong>Model 4:</strong> Democratic Seat Share using midterm years
yhat = 46.38% , 95% CI (42.95, 49.82)
y = 49.30%</p>
<p>To start assessing the post-election accuracy of these models and reflect on their prediction capabilities, I have created four graphs below with the actual versus predicted results each year. In my Final Prediction Blog, these graphs had the 2022 predictions as a point along the regression line of the model with both the x and y value equaling the model’s prediction for the 2022 election. If its prediction had been completely accurate, this point would have stayed along the regression line. In the plots below, I have added a blue point that represents the actual election results as of November 21st, 2022, when 430 of the House seats have been decided. According to the Cook Political Report, Democrats have won 212 seats at this point, and Republicans have won 218 - a democratic seat share of 49.30%. The Cook Political Report also has counted a total of 50,464,746 votes for Democrats versus 53,948,354 for Republicans - giving a democratic vote share of 47.59%. In the graphs below, our predicted values have not changed since the models have not been adjusted at all from the final prediction, however the new y-value of the point shows the actual outcome. The distance between this point and the regression line shows the accuracy of our prediction, with a point closer to the line representing results that came very close to our prediction, and points further from the line representing a less-accurate prediction.</p>
<div id="model-1" class="section level4">
<h4>Model 1</h4>
<p><code>$$ Democratic Vote Share = \beta_0 + Generic Ballot Democrat\beta_1 + Lag House Dem Vote Share\beta_2 + President Party R\beta_3 + President Approval\beta_4 + Midterm\beta_5 + Unemployment Rate\beta_6 + President Party R * Midterm\beta_7 + President Party R * Unemployment\beta_8 $$</code></p>
<p><img src="http://example.org/post/2022-11-18-post-election-reflection/index_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The graph above shows the model for vote share across all election years, both midterm and presidential. As we can see, the predicted vote share for the 2022 House was 50.2022%, or practically a tie between the Democrats and Republicans. We know that in recent history it is not uncommon for the vote share to be relatively close, even if one party comes away winning the majority of the seats – or in the case of a presidential election, the electoral votes – so this prediction seemed reasonable prior to the election. The actual democratic vote share was around 47.6%, which fell just outside of my prediction’s 95% Confidence Interval of (47.8, 52.54) that is represented by the grey dotted line on the plot. The fact that the actual vote share was <em>not</em> in the 95% confidence interval puzzles me a bit, as it makes me question whether this election fell to an extreme, or if this model was not the best predictor of the election since we would have hoped the error margins would be wide enough to capture this result. Despite these questions and falling outside of the confidence interval, my prediction is actually not that far off from the actual results, overestimating the vote share by 2.6122 percentage points.</p>
</div>
<div id="model-2" class="section level4">
<h4>Model 2</h4>
<p><code>$$ Democratic Seat Share = \beta_0 + Generic Ballot Democrat\beta_1 + Lag House Dem Seat Share\beta_2 + President Party R\beta_3 + President Approval\beta_4 + Midterm\beta_5 + Unemployment Rate\beta_6 + President Party R * Midterm\beta_7 + President Party R * Unemployment\beta_8 $$</code></p>
<p><img src="http://example.org/post/2022-11-18-post-election-reflection/index_files/figure-html/unnamed-chunk-4-1.png" width="672" />
This model of seat share using data across all the election years fared much better than the one above, and produced a prediction that was very close to the results we actually saw, with the model overestimating the seat share by just 1.21 percentage points. In this model, the results also fell within the 95% confidence interval, unlike above. In my final election prediction blog, I noted that this model had the highest R-squared value, and these results resemble that accuracy.</p>
</div>
<div id="model-3" class="section level4">
<h4>Model 3</h4>
<p><code>$$ Democratic Vote Share = \beta_0 + Generic Ballot Democrat\beta_1 + Lag House Dem Vote Share\beta_2 + President Party R\beta_3 $$</code></p>
<p><img src="http://example.org/post/2022-11-18-post-election-reflection/index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>In the graph above, this model used only the midterm election years to regress on Democratic vote share and the results were extremely accurate. This model also had a high R-squared of 0.89, and it only overestimated the results by 0.27 percentage points. This was the most accurate prediction out of all four models, which surprised me a bit. In my initial predictions, I thought that these midterm year models would be less predictive, as they have fewer data points to work off of, and included fewer independent variables. While this midterm model was an accurate predictor, the other midterm model did not perform as well.</p>
</div>
<div id="model-4" class="section level4">
<h4>Model 4</h4>
<p><code>$$ Democratic Seat Share = \beta_0 + Generic Ballot Democrat\beta_1 + Lag House Dem Seat Share\beta_2 + President Party R\beta_3 $$</code></p>
<p><img src="http://example.org/post/2022-11-18-post-election-reflection/index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>This midterm model underestimated the actual democratic seat share of the election by 2.92 percentage points. This was the only model that <em>underestimated</em> the dependent variable, however the prediction did still fall within the 95% confidence interval. This was the largest percentage-point difference between the prediction and the result across all four of the models, but I did expect this as the R-squared value is also the lowest of the four.</p>
<p>Looking at the results across all four models, it seems that using only the midterm years’ model for democratic vote share is more predictive than if we had used the model that spans all elections, while we see the opposite for democratic seat share - with the more predictive model being the one that uses all the elections. One reason I speculate this may be is because there is a different voter population during midterm years than election years, which has a strong affect on the vote share while the seats are more representative of the areas demographics. The number of voters in midterm years differs dramatically from the number that vote in election years - with presidential election years generating turnouts of around 50-60% and midterm elections only generating about 40% turnout. Since the number of voters participating differs between the two, it may be harder for the models that use all the years’ data to accurately predict the vote share down to a hundredth of a percentage point, as we are looking at here. If this speculation is somewhat correct, this would mean that incorporating voter turnout in to the model may have improved the accuracy. I will check this below by running the vote share model across all years and adding turnout as an independent variable to see if this improves the accuracy of the model.</p>
</div>


		
	</div>

	<div class="pagination">
		<a href="/post/2022-11-04-final-prediction/" class="left arrow">&#8592;</a>

		<a href="#" class="top">Top</a>
	</div>
</main>


        		<footer>
			
			<span>
			&copy; <time datetime="2022-11-21 23:37:48.377064 -0500 EST m=&#43;2.160726853">2022</time> . Made with <a href='https://gohugo.io'>Hugo</a> using the <a href='https://github.com/EmielH/tale-hugo/'>Tale</a> theme.
			</span>
		</footer>

    </body>
</html>
