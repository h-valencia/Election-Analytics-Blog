---
title: Post-Election Reflection
author: Hannah Valencia
date: '2022-11-18'
slug: []
categories: []
tags: []
summary: "On Tuesday, November 8th, eligible Americans voted in the midterm elections for candidates who will make up the 118th Congress. Prior to the election, we had created forecasting models in an attempt to determine who would win the House of Representatives, at the district-level, national-level, or both. In this post I will reflect on the predictions made, my model's performance, and comment on the election overall."
---

*This blog is part of a series related to Gov 1347: Election Analytics, a course at [Harvard University](https://www.harvard.edu/) taught by Professor [Ryan D. Enos](http://ryandenos.com/)*.

*On Tuesday, November 8th, eligible Americans voted in the midterm elections for candidates who will make up the 118th Congress. Prior to the election, we had created forecasting models in an attempt to determine who would win the House of Representatives, at the district-level, national-level, or both. In this post I will reflect on the predictions made, my model's performance, and comment on the election overall.*

Using the knowledge I gained this semester about election forecasting, I created multiple models to predict the outcome of the 2022 House of Representatives election. I looked at both seat share and vote share, and created two models for each: one using data across all election years, and one using the data for only midterm years. We had the opportunity to work with both national-level and seat-level data, but all of my final models used the data at the national-level for reasons I outlined in my [Final Prediction Blog](https://h-valencia.github.io/Election-Analytics-Blog/post/2022-11-04-final-prediction/).

As a refresher, here are the predictions from my four models and the actual results of the election as of November 21st, 2022:
**Model 1:** Democratic Vote Share using all years
$\hat{y}$ = 50.20% , 95% CI (47.86, 52.54)
$y$ = 47.59%

**Model 2:** Democratic Seat Share using all years
$\hat{y}$ = 50.51% , 95% CI (45.71, 55.31)
$y$ = 49.30%

**Model 3:** Democratic Vote Share using midterm years
$\hat{y}$ = 47.86% , 95% CI (45.81, 49.90)
$y$ = 47.59%

**Model 4:** Democratic Seat Share using midterm years
$\hat{y}$ = 46.38% , 95% CI (42.95, 49.82)
$y$ = 49.30%

To start assessing the post-election accuracy of these models and reflect on their prediction capabilities, I have created four graphs below with the actual versus predicted results each year. In my Final Prediction Blog, these graphs had the 2022 predictions as a point along the regression line of the model with both the x and y value equaling the model's prediction for the 2022 election. If its prediction had been completely accurate, this point would have stayed along the regression line. In the plots below, I have added a blue point that represents the actual election results as of November 21st, 2022, when 430 of the House seats have been decided. According to the Cook Political Report, Democrats have won 212 seats at this point, and Republicans have won 218 - a democratic seat share of 49.30%. The Cook Political Report also has counted a total of 50,464,746 votes for Democrats versus 53,948,354 for Republicans - giving a democratic vote share of 47.59%. In the graphs below, our predicted values have not changed since the models have not been adjusted at all from the final prediction, however the new y-value of the point shows the actual outcome. The distance between this point and the regression line shows the accuracy of our prediction, with a point closer to the line representing results that came very close to our prediction, and points further from the line representing a less-accurate prediction. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# load libraries

library(tidyverse)
library(usmap)
library(plotly)
library(rmapshaper)
library(sf)
library(haven)
library(stringr)
library(sjlabelled)
library(usdata)
library(ggpubr)
library(stargazer)

```

```{r}
natldata <- read_csv("natldata2.csv")
```


#### Model 1
```{r}
# prediction vs actual 2022
# Dem VS all years plot

natldata %>%
  ggplot(aes(x = prediction1, y = D_majorvote_pct, label = year)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  geom_text(hjust=0, vjust=0, size = 3) +
  labs(x = "Predicted Democratic Vote Share (all years)",
       y = "Actual Democratic Vote Share",
       title = "Actual vs. Predicted Democratic Vote Share (all years)",
       caption = "Results from Cook Political Report as of 11/21/2022.") +
  stat_cor(method="pearson") +
  geom_point(aes(x=50.20221, y=50.20221, label = "Prediction"), colour="grey75") +
  geom_text(aes(x=50.20221, y=50.7, label = "Prediction"), color = "grey75", size = 3.5) +
  geom_point(aes(x=50.20221, y=47.58511605, label = "2022 Results"), colour="dodgerblue3") +
  geom_text(aes(x=50.20221, y=48.1, label = "2022 Results"), color = "dodgerblue3", size = 3.5) +
  geom_segment(aes(x=50.20221,y=47.86202,xend=50.20221,yend=52.5424), color = "grey80", linetype = "dotted")
```

The graph above shows the model for vote share across all election years, both midterm and presidential. As we can see, the predicted vote share for the 2022 House was 50.2022%, or practically a tie between the Democrats and Republicans. We know that in recent history it is not uncommon for the vote share to be relatively close, even if one party comes away winning the majority of the seats -- or in the case of a presidential election, the electoral votes -- so this prediction seemed reasonable prior to the election. The actual democratic vote share was around 47.6%, which fell just outside of my prediction's 95% Confidence Interval of (47.8, 52.54) that is represented by the grey dotted line on the plot. The fact that the actual vote share was *not* in the 95% confidence interval puzzles me a bit, as it makes me question whether this election fell to an extreme, or if this model was not the best predictor of the election since we would have hoped the error margins would be wide enough to capture this result. Despite these questions and falling outside of the confidence interval, my prediction is actually not that far off from the actual results, overestimating the vote share by 2.6122 percentage points.

```{r}
# maybe make a residual plot?

# natldata %>%
#   mutate(p1resid = prediction1 - D_majorvote_pct) %>%
#   ggplot(aes(x = p1resid)) +
#   geom_histogram(binwidth = 1, fill = "dodgerblue2", color = "black") +
#   geom_vline(xintercept = (50.20221 - 47.58511605), color = "red", linetype = "dashed") +
#   labs(x = "Residuals of Predicted Vote Shares",
#        y = "Frequency",
#        title = "")
```


#### Model 2
```{r}
# Dem Seat Share all years plot

natldata %>%
  ggplot(aes(x = prediction2, y = DemSeatShare, label = year)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  geom_text(hjust=0, vjust=0, size = 3) +
  labs(x = "Predicted Democratic Seat Share (all years)",
       y = "Actual Democratic Seat Share",
       title = "Actual vs. Predicted Democratic Seat Share (all years)",
       caption = "Results from Cook Political Report as of 11/21/2022 when 430 seats have been decided") +
  stat_cor(method="pearson") +
  geom_point(aes(x=50.50952, y=50.50952, label = "Prediction"), colour="grey75") +
  geom_text(aes(x=50.25, y=51.25, label = "Prediction"), color = "grey75", size = 3.5) +
  geom_point(aes(x=50.50952, y=49.30232558, label = "2022 Results"), colour="dodgerblue3") +
  geom_text(aes(x=50.25, y=48.5, label = "2022 Results"), color = "dodgerblue3", size = 3.5) +
  geom_segment(aes(x=50.50952,y=45.7082,xend=50.50952,yend=55.31085), color = "grey80", linetype = "dotted")

```
This model of seat share using data across all the election years fared much better than the one above, and produced a prediction that was very close to the results we actually saw, with the model overestimating the seat share by just 1.21 percentage points. In this model, the results also fell within the 95% confidence interval, unlike above. In my final election prediction blog, I noted that this model had the highest R-squared value, and these results resemble that accuracy. 



```{r}
# Dem VS midterm years plot

natldata %>%
  filter(midterm == 1) %>%
  ggplot(aes(x = prediction3, y = D_majorvote_pct, label = year)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  geom_text(hjust=0, vjust=0, size = 3) +
  labs(x = "Predicted Democratic Vote Share (midterm years)",
       y = "Actual Democratic Vote Share",
       title = "Actual vs. Predicted Democratic Vote Share (midterm years)",
       caption = "Results from Cook Political Report as of 11/21/2022.") +
  stat_cor(method="pearson") +
  geom_point(aes(x=47.85624, y=47.85624, label = "Prediction"), colour="grey75") +
  geom_text(aes(x=47.75, y=48.25, label = "Prediction"), color = "grey75", size = 3.5) +
  geom_point(aes(x=47.85624, y=47.58511605, label = "2022 Results"), colour="dodgerblue3") +
  geom_text(aes(x=48.8, y=47.6, label = "2022 Results"), color = "dodgerblue3", size = 3.5)

```

```{r}

# Dem Seat Share midterm years plot

natldata %>%
  filter(midterm == 1) %>%
  ggplot(aes(x = prediction4, y = D_majorvote_pct, label = year)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  geom_text(hjust=0, vjust=0, size = 3) +
  labs(x = "Predicted Democratic Seat Share (midterm years)",
       y = "Actual Democratic Seat Share",
       title = "Actual vs. Predicted Democratic Seat Share (midterm years)",
       caption = "Results from Cook Political Report as of 11/21/2022 when 430 seats have been decided") +
  stat_cor(method="pearson") +
  geom_point(aes(x=46.38345, y=46.38345, label = "Prediction"), colour="grey75") +
  geom_text(aes(x=45.9, y=46.8, label = "Prediction"), color = "grey75", size = 3.5) +
  geom_point(aes(x=46.38345, y=49.30232558, label = "2022 Results"), colour="dodgerblue3") +
  geom_text(aes(x=47.5, y=49.8, label = "2022 Results"), color = "dodgerblue3", size = 3.5)

```































