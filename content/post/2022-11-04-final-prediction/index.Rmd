---
title: Final Prediction
author: Hannah Valencia
date: '2022-11-04'
slug: []
categories: []
tags: []
summary: "Over the past nine weeks, we have explored a multitude of variables that may impact election outcomes in an attempt to forecast the 2022 midterm election. We have learned about economic forces, polling, expert predictions, incumbency, advertising, campaigning, and more to find if they hold any predictive power, and how together in a model they may foreshadow what is to come in the results next week. In this blog, I will create a final prediction model for the House of Representatives and share its results, with the intention of reflecting on it after the election." 
---

*This blog is part of a series related to Gov 1347: Election Analytics, a course at [Harvard University](https://www.harvard.edu/) taught by Professor [Ryan D. Enos](http://ryandenos.com/)*.

*Over the past nine weeks, we have explored a multitude of variables that may impact election outcomes in an attempt to forecast the 2022 midterm election. We have learned about economic forces, polling, expert predictions, incumbency, advertising, campaigning, and more to find if they hold any predictive power, and how together in a model they may foreshadow what is to come in the results next week. In this blog, I will create a final prediction model for the House of Representatives and share its results, with the intention of reflecting on it after the election.*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# load libraries

library(tidyverse)
library(usmap)
library(plotly)
library(rmapshaper)
library(sf)
library(haven)
library(stringr)
library(sjlabelled)
library(usdata)
library(ggpubr)
library(stargazer)

```

```{r}

# reading in data

distdata <- read_csv("fulldata.csv") # master df by district
natldata <- read_csv("data2.csv", skip = 1) # master df 
popvote_df <- read_csv("popvote_df.csv", skip = 1, col_types = cols(...1 = col_skip(), ...2 = col_skip()))
rdi_df <- read_csv("RDI_quarterly.csv") %>% select(!`...1`)
qtnatl <- read_csv("data1.csv") %>% select(!c(`...1`, `...2`))
gallup_approval <- read_csv("Gallup approval.csv")
```

### Background

Since this class first convened in the beginning of September, we have time and time again been reminded of the shortcomings of election forecasting. Elections are fickle and there is no way to know for certain how voters will vote on election day. The experts oftentimes cannot predict what will happen, if an election will be toss-up or a landslide. There are shocks that no one sees coming, costs of voting that may be unanticipated, various biases with polling, and ever-changing political opinions that result from all sorts of issues and news. While these unknowns make forecasting very difficult, we have gained the tools over the last nine weeks that give us a good foundation to create predictions of our own for the House of Representatives midterm election.

### My Model Choices

In building my model, the first choice I had was what to use as my dependent variable and the level on which to predict on. For my dependent variable, I decided to run models for both Democratic seat share and Democratic vote share to see the results of both and how they may unveil different stories about the election. I also created variables putting the vote share and seat share in terms of the incumbent party to see how incumbents perform without paying specific attention to their party affiliation, but I decided that this approach would complicate interpreting results. While we spent multiple weeks working with district-level data and forecasting each of the 435 districts, in the end I have decided to predict vote share and seat share on the national level. We have more data for these variables and do not have to rely upon things like pooling, that may produce greater margins for error. Given the insufficient district-level data, a pooled model would have allowed us to take data from neighboring, similar districts and count it as its own. However, my national model was heading in the right direction in terms of improving predictability, and the lack of district data led me to decide on a national-level final model. 

In the first week of class, we looked at the fundamentals of election forecasting models, with predictors like the president's party. We learned about the phenomenon that takes place in most midterms where the president's party tends to lose seats [(Campbell 2018)](https://www.cambridge.org/core/journals/ps-political-science-and-politics/article/abs/introduction-forecasting-the-2018-us-midterm-elections/6BA191A0140468E022ECCE67D1CDA03B). Following these basics, in week two we looked at how Real Disposable Income (RDI) on its own does a decent job of predicting elections. We also took a look at a slew of other economic variables, such as the Consumer Price Index (CPI), GDP growth rate, unemployment, and more. Despite the overlap between the government and the economy, most of these variables proved to be insignificant and poor predictors for the House elections. The one economic variable I will be using in my final model the unemployment rate, as it is typically a decent indicator of the health of the economy. In addition to using it on its own, I have also decided to interact it with the president's party. The most important topics to voters vary based on their political party affiliation, and Republicans are known to weigh economic factors more heavily. Therefore, an interaction term between party and unemployment rate can provide us with more significant results than the two can separately on their own.

We continued on from the fundamentals and economic variables onto polling, looking at the generic ballot as well as district-level polls. When adding these polls to my model, I had limited the data to 1990 and later, justifying this with the fact that polling methods have greatly changed since 1945 when the data was first captured. It was my hope that in reducing the sample, the model would better predict recent elections. While I think this justification still makes sense, especially for district-level polling where there does not exist much data even today, I will not be limiting the data by year as severely. In my final model, the only polling data I will be using is the generic ballot, measuring which party people support without regard for specific candidates. The data I will be using goes back to 1960, which is when all the variables of my interest have data for. It would still make sense to limit the data to more recent years for the same polling reasons, but since I am only dealing with the generic ballot, I believe the changing polling methods will not hinder the predictability of the model and it will be more useful to have the longer data.

While we did not dedicate much time in class to discussing presidential approval ratings and their impact on elections, some classmates and I spoke about incorporating it as another form of polling that has easily accessible historical data. I gathered data on presidential approval from [Gallup Analytics](https://analyticscampus-gallup-com.ezp-prod1.hul.harvard.edu/Tables) that date back to 1960. Polling dates varied year to year, so I decided pulled the president's approval rating from whichever the last poll right before the election was. A few years in the 1960s and 1970s, this ended up being polls that were taken from June-August of the election year, but for most other years and especially in more recent history, the ratings are from the final weeks of October. While it would have been nice to have a specific date that the approval rating was pulled from consistently every year, I believe using the poll prior to the election will mitigate any issue.

Incumbency has also been a popular word throughout our class, and it is proven as a good predictor for the House of Representatives elections. Looking at the district-level data we have from 1945-2020, we find that the incumbent wins the House race about 84% of the time. Below is a bar graph showing this relationship.

```{r}
distdata %>%
  group_by(winner_candidate_inc) %>%
  summarize(count = n()) %>%
  mutate(pct_inc = count/sum(count)) %>%
  ggplot(aes(x = winner_candidate_inc, y = pct_inc, fill = winner_candidate_inc)) +
  geom_bar(stat = "identity", fill = c("dodgerblue1", "dodgerblue4")) +
  labs(x = "Winning Candidate Incumbency Status",
       y = "Percent",
       title = "Percent of Incumbents That Win House Elections (1945-2020)") +
  theme(legend.position = "none")
```

While this data is important at the district level, we learned about other incumbency phenomena that take place at a national level, such as the house flipping at midterm elections and the President's party losing seats. Despite this incumbency data surely boasting high predictive power at the district-level, it does not provide much to us on a national-level. In a similar vein, however, rather than looking at the House majority incumbent from the year before, we can use the previous election's vote share and seat share to predict that of the following election. I created lagged variables for both of these variables and will incorporate them into my final model, along with the President's party variable that I previously mentioned.

The final variable I will be using in my model is an indicator variable for if the election takes place in a midterm year or not. From our discussions and readings, we know that different types of voters come out for midterm elections than presidential elections. We also learned in class that many Americans do not even know who their representative is. Those who vote in a midterm year are likely more involved politically and stay up-to-date with the news. I also will be interacting this term with the president's party variable with the thought that it could account for some of the seat flipping that we historically see. Since 1955, every house flip has been the result of a midterm election [(Reuters)](https://www.reuters.com/graphics/USA-ELECTION/MIDTERMS/gdpzyzowgvw/index.html). This interaction term can account for the additional incumbent losses that occur during midterms but not during election years.

Throughout our time this semester, we have also worked with multiple other district-level variables that I will not be including in my model. We learned about advertising and the amount parties spend on advertising. In [Gerber et al. 2011](https://hollis.harvard.edu/primo-explore/fulldisplay?docid=TN_cdi_proquest_miscellaneous_881466543&context=PC&vid=HVD2&search_scope=everything&tab=everything&lang=en_US) the authors found that "televised ads have strong but short-lived effects on voting preferences". With this in mind, along with in-class discussions about advertising data being unreliable and sometimes unavailable, I decided to forego using it in my model. Other district-level variables such as the cost of voting, ground campaigning efforts, and voter turnout I will not be adjusting to incorporate into my national model.

### Formulas

Given all the data we have considered using this semester, and after many rounds of trial and error, I have created two final models:

$$
Democratic Vote Share = \beta_0 + Generic Ballot Democrat\beta_1 + Lag House Dem Vote Share\beta_2 + President Party R\beta_3 + President Approval\beta_4 + Midterm\beta_5 + Unemployment Rate\beta_6 + President Party R * Midterm\beta_7 + President Party R * Unemployment\beta_8
$$

$$
Democratic Seat Share = \beta_0 + Generic Ballot Democrat\beta_1 + Lag House Dem Seat Share\beta_2 + President Party R\beta_3 + President Approval\beta_4 + Midterm\beta_5 + Unemployment Rate\beta_6 + President Party R * Midterm\beta_7 + President Party R * Unemployment\beta_8
$$

```{r}

# recreating Professor Enos's graph from week 1 illustrating the relationship between percent change RDI and the seat change in the president's party

# adding seat share and president party to the data frame with national-level data

a <- distdata %>%
  distinct(year, president_party)

b <- popvote_df %>%
  select(year, R_seats, D_seats, Other_seats)

natldata1 <- left_join(natldata, a, by = c("year" = "year"))
natldata1 <- left_join(natldata1, b, by = c("year" = "year")) %>% select(!`...1`)

# lagging the seats to create a variable that has the seat change for the President's party

c <- b %>%
  mutate(year = year + 2) %>%
  rename(R_seats_previous = R_seats,
         D_seats_previous = D_seats,
         Other_seats_previous = Other_seats)

natldata1 <- left_join(natldata1, c, by = c("year" = "year"))

natldata1 <- natldata1 %>%
  mutate(pres_party_seat_change = ifelse(president_party == "R", (R_seats - R_seats_previous), (D_seats - D_seats_previous)))

# creating a variable for the percent change in RDI for each election year
# coded as the percent change in RDI from Q3 of the election year (aka Q7) from the RDI from Q3 of the previous year

rdi_df1 <- rdi_df %>%
  mutate(election_year = 1:nrow(rdi_df) %/% 8,
         election_year = lag(election_year, default = 0)) %>%
  group_by(election_year) %>%
  filter(election_year != 31) %>%
  mutate(pct_rdi_election = (DSPIC_qt[quarter_cycle == 7] - DSPIC_qt[quarter_cycle == 3])/DSPIC_qt[quarter_cycle == 3]*100)

rdi_df1 <- rdi_df1 %>%
  distinct(year, pct_rdi_election)

natldata1 <- left_join(natldata1, rdi_df1, by = c("year" = "year"))

# For some reason the graph I produce is different than that of Professor Enos's, likely due to slight differences in the way we coded the variables.
# Even though I will not include the recreated graph in this post, I have the same new variables that I can possibly incorporate into a model or graph later on.

# x <- natldata1 %>% select(year, pres_party_seat_change, pct_rdi_election)

# natldata1 %>%
#   ggplot(aes(x = pct_rdi_election, y = pres_party_seat_change, label = year)) +
#   geom_point() +
#   geom_smooth(method = "lm", se = TRUE, alpha = .25,size = 2) +
#   labs(title = "Descriptive Relationship Between RDI Change and \n President's Party Seat Change in House",
#        x = "Percent Change in RDI",
#        y = "President's Party Seat Change",
#        caption = "RDI = Real Disposable Income, Midterm years in black, \n change from Q3 of election year - 1 year") +
#   scale_color_manual(values=c("darkgrey","black")) +
#   scale_y_continuous(breaks = seq(-60, 40, by = 10)) +
#   theme(legend.position = "none")
```

```{r}

d <- distdata %>%
  select(year, gen_avg_dem, gen_avg_rep) %>%
  distinct(year, gen_avg_dem, gen_avg_rep)

e <- qtnatl %>%
  distinct(year, R_majorvote_pct, D_majorvote_pct)

natldata1 <- left_join(natldata1, d, by = c("year" = "year"))
natldata1 <- left_join(natldata1, e, by = c("year" = "year"))

# creating two new dependent variables in case I want to try them out
# vote share and seat share for the president's party rather than just the democratic and republican ones

natldata1 <- natldata1 %>%
  mutate(pres_party_vote_pct = ifelse(president_party == "D", D_majorvote_pct, R_majorvote_pct),
         pres_party_seat_share = ifelse(president_party == "D", DemSeatShare, RepSeatShare))

# creating a variable for if the year is a midterm election year

natldata2 <- natldata1
natldata2$midterm <- seq_len(nrow(natldata2)) %% 2L + 1L
natldata2$midterm[natldata2$midterm == 2] <- 0

# adding presidential approval ratings for each year from Gallup Analytics

natldata2 <- left_join(natldata2, gallup_approval, by = c("year" = "year"))

# adding lagged vote share
f <- natldata2 %>%
  mutate(year = year + 2,
         DemVS_prev = D_majorvote_pct) %>%
  select(year, DemVS_prev)

# googled the seat share for 1958 to fill in for 1960's lag

natldata2 <- left_join(natldata2, f, by = c("year" = "year"))
natldata2$DemVS_prev[natldata2$year == 1960] <- 56.0

# adding lagged seat share to get same effect as the lagged vote share but for the seat share model
g <- natldata2 %>%
  mutate(year = year + 2,
         DemSeatShare_prev = DemSeatShare) %>%
  select(year, DemSeatShare_prev)

# googled the seat share for 1958 to fill in for 1960's lag

natldata2 <- left_join(natldata2, g, by = c("year" = "year"))
natldata2$DemSeatShare_prev[natldata2$year == 1960] <- (283/435)

# changing the format of the seat share variable to match that of vote share (ex. 56 instead of 0.56)
natldata2 <- natldata2 %>%
  mutate(DemSeatShare = DemSeatShare * 100)


```

```{r, results='asis'}

# modelling Democratic vote share

fit1a <- lm(data = natldata2, D_majorvote_pct ~ gen_avg_dem)
#summary(fit1a) 

fit1b <- lm(data = natldata2, D_majorvote_pct ~ gen_avg_dem + DemVS_prev)
#summary(fit1b)

fit1c <- lm(data = natldata2, D_majorvote_pct ~ gen_avg_dem + DemVS_prev + president_party)
#summary(fit1c)

fit1d <- lm(data = natldata2, D_majorvote_pct ~ gen_avg_dem + DemVS_prev + president_party + approval_rating)
#summary(fit1d)

fit1e <- lm(data = natldata2, D_majorvote_pct ~ gen_avg_dem + DemVS_prev + president_party + approval_rating + midterm*president_party)
#summary(fit1e)

fit1f <- lm(data = natldata2, D_majorvote_pct ~ gen_avg_dem + DemVS_prev + president_party + approval_rating + midterm*president_party + unrate + unrate*president_party)
# summary(fit1f)

stargazer(fit1a, fit1b, fit1c, fit1d, fit1e, fit1f, type = "html", covariate.labels = c("Generic Ballot Support - D", "Lag House Dem VS", "President Party - R", "President Approval Rating", "Midterm Year", "Unemployment Rate", "President Party - R * Midterm Year", "President Party - R * Unemployment", "Constant"))

```


```{r, results = 'asis'}
# modelling Democratic seat share

fit2a <- lm(data = natldata2, DemSeatShare ~ gen_avg_dem)
#summary(fit2a) 

fit2b <- lm(data = natldata2, DemSeatShare ~ gen_avg_dem + DemSeatShare_prev)
#summary(fit2b)

fit2c <- lm(data = natldata2, DemSeatShare ~ gen_avg_dem + DemSeatShare_prev + president_party)
#summary(fit2c)

fit2d <- lm(data = natldata2, DemSeatShare ~ gen_avg_dem + DemSeatShare_prev + president_party + approval_rating)
#summary(fit2d)

fit2e <- lm(data = natldata2, DemSeatShare ~ gen_avg_dem + DemSeatShare_prev + president_party + approval_rating + midterm*president_party)
#summary(fit2e)

fit2f <- lm(data = natldata2, DemSeatShare ~ gen_avg_dem + DemSeatShare_prev + president_party + approval_rating + midterm*president_party + unrate + unrate*president_party)
#summary(fit2f)

stargazer(fit2a, fit2b, fit2c, fit2d, fit2e, fit2f, type = "html", covariate.labels = c("Generic Ballot Support - D", "Lag House Dem Seat Share", "President Party - R", "President Approval Rating", "Midterm Year", "Unemployment Rate", "President Party - R * Midterm Year", "President Party - R * Unemployment", "Constant"))

```


```{r, results = 'asis'}

# trying new models only using midterm years

midtermdata <- natldata2 %>%
  filter(midterm == 1)

fit3a <- lm(data = midtermdata, D_majorvote_pct ~ gen_avg_dem)
#summary(fit3a) 

fit3b <- lm(data = midtermdata, D_majorvote_pct ~ gen_avg_dem + president_party)
#summary(fit3b)

fit3c <- lm(data = midtermdata, D_majorvote_pct ~ gen_avg_dem + president_party + DemVS_prev)
#summary(fit3c)

stargazer(fit3a, fit3b, fit3c, type = "html", covariate.labels = c("Generic Ballot Support - D", "President Party - R", "Lag House Dem VS"))


# seat share model for only midterm years

fit4a <- lm(data = midtermdata, DemSeatShare ~ gen_avg_dem)
#summary(fit4a) 

fit4b <- lm(data = midtermdata, DemSeatShare ~ gen_avg_dem + president_party)
#summary(fit4b)

fit4c <- lm(data = midtermdata, DemSeatShare ~ gen_avg_dem + president_party + DemSeatShare_prev)
#summary(fit4c)

stargazer(fit4a, fit4b, fit4c, type = "html", covariate.labels = c("Generic Ballot Support - D", "President Party - R", "Lag House Dem Seat Share"))

```

```{r}

# creating a dataframe with the 2022 independent variables data

data2022 <- data.frame(gen_avg_dem = 45.5, # fivethirtyeight
                       DemVS_prev = natldata2$D_majorvote_pct[natldata2$year == 2020],
                       DemSeatShare_prev = natldata2$DemSeatShare[natldata2$year == 2020],
                       president_party = "D",
                       approval_rating = 0.40, # gallup
                       midterm = 1,
                       unrate = 3.7) # bls

# predicting the 2022 election using each model

predict(fit1f, data2022, interval = "confidence") # Democratic Vote Share using all years
predict(fit2f, data2022, interval = "confidence") # Democratic Seat Share using all years
predict(fit3c, data2022, interval = "confidence") # Democratic Vote Share using midterm years
predict(fit4c, data2022, interval = "confidence") # Democratic Seat Share using midterm years
```

```{r}
# in sample testing
# creating empty columns for the predictions of each of my four final models

natldata2$prediction1 <- NA
natldata2$prediction2 <- NA
natldata2$prediction3 <- NA
natldata2$prediction4 <- NA

# creating a for loop that fills in the data frame with the predictions for each model for every year

for (i in 1960:2020){
  x <- natldata2 %>%
    filter(year == i) %>%
    select(gen_avg_dem,DemVS_prev,DemSeatShare_prev, president_party, approval_rating, midterm, unrate)
  
  natldata2$prediction1[natldata2$year == i] <- predict(fit1f, x, interval = "confidence")[1]
  natldata2$prediction2[natldata2$year == i] <- predict(fit2f, x, interval = "confidence")[1] 
  natldata2$prediction3[natldata2$year == i] <- predict(fit3c, x, interval = "confidence")[1] 
  natldata2$prediction4[natldata2$year == i] <- predict(fit4c, x, interval = "confidence")[1] 
}

```

```{r}

# Dem VS all years plot

natldata2 %>%
  ggplot(aes(x = prediction1, y = D_majorvote_pct, label = year)) +
  geom_point() +
  geom_abline(y = x, color = "red") +
  geom_text(hjust=0, vjust=0, size = 3) +
  labs(x = "Precited Democratic Vote Share (all years)",
       y = "Actual Democratic Vote Share",
       title = "Actual vs. Predicted Democratic Vote Share (all years)") +
  stat_cor(method="pearson")

```

```{r}
# Dem Seat Share all years plot

natldata2 %>%
  ggplot(aes(x = prediction2, y = DemSeatShare, label = year)) +
  geom_point() +
  geom_abline(y = x, color = "red") +
  geom_text(hjust=0, vjust=0, size = 3) +
  labs(x = "Precited Democratic Seat Share (all years)",
       y = "Actual Democratic Seat Share",
       title = "Actual vs. Predicted Democratic Seat Share (all years)") +
  stat_cor(method="pearson")
```

```{r}

# Dem VS midterm years plot

natldata2 %>%
  filter(midterm == 1) %>%
  ggplot(aes(x = prediction3, y = D_majorvote_pct, label = year)) +
  geom_point() +
  geom_abline(y = x, color = "red") +
  geom_text(hjust=0, vjust=0, size = 3) +
  labs(x = "Precited Democratic Vote Share (midterm years)",
       y = "Actual Democratic Vote Share",
       title = "Actual vs. Predicted Democratic Vote Share (midterm years)") +
  stat_cor(method="pearson")

```

```{r}

# Dem Seat Share midterm years plot

natldata2 %>%
  filter(midterm == 1) %>%
  ggplot(aes(x = prediction4, y = D_majorvote_pct, label = year)) +
  geom_point() +
  geom_abline(y = x, color = "red") +
  geom_text(hjust=0, vjust=0, size = 3) +
  labs(x = "Precited Democratic Seat Share (midterm years)",
       y = "Actual Democratic Seat Share",
       title = "Actual vs. Predicted Democratic Seat Share (midterm years)") +
  stat_cor(method="pearson")

```

```{r}

# Maine District 2 prediction ?

maine <- distdata %>%
  filter(district_id == "ME02") %>%
  distinct(year, .keep_all = TRUE) %>%
  filter(year >= 1980)
  
```

































